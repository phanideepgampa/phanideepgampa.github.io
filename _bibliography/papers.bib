---
---
@ARTICLE{2019arXiv190611245G,
       author = {{Gampa}, Phanideep and {Satwik Kondamudi}, Sairam and
         {Kailasam}, Lakshmanan},
        title = "{A Tractable Algorithm For Finite-Horizon Continuous Reinforcement Learning}",
      journal = {International Conference on Intelligent Autonomous Systems (ICoIAS), Singapore},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
         year = "2019",
        month = "Feb",
        arxiv = "1906.11245",
          eid = {arXiv:1906.11245},
        pages = {arXiv:1906.11245},
archivePrefix = {arXiv},
       eprint = {1906.11245},
 primaryClass = {cs.LG},
      abstract = {We consider the finite horizon continuous reinforcement learning problem. Our contribution is three-fold.
First,we give a tractable algorithm based on optimistic value iteration for the problem.
Next,we give a lower bound on regret of order $\Omega(T^{2/3})$ for any algorithm that discretizes the state space,
improving the previous regret bound of $\Omega(T^{1/2})$ of Ortner and Ryabko \cite{contrl} for the same problem. Next,under the assumption that the rewards and transitions are H\"{o}lder Continuous 
we show that the upper bound on the discretization error is $const.Ln^{-\alpha}T$. 
Finally,we give some simple experiments to validate our propositions.
},
     code= {https://github.com/phanideepgampa/Optimal-regret-bounds-for-continuous-rl},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190611245G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

